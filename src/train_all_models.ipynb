{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, XLNetTokenizer, XLNetForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import os\n",
    "import time\n",
    "from classes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to mark differnt versions of models as script is updated\n",
    "version = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert-base-uncased\n",
    "#distilbert-base-uncased\n",
    "#xlnet-base-uncased\n",
    "#roberta-base\n",
    "#distilroberta-base\n",
    "\n",
    "# List which models to train\n",
    "model_list = ['xlnet-base-uncased','bert-base-uncased','distilbert-base-uncased',\n",
    "              'roberta-base','distilroberta-base']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "train_raw,test_raw,train_label_raw,test_label_raw=[],[],[],[]\n",
    "with open('../input/train.ft.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "for line in tqdm(lines):\n",
    "    train_raw.append(line.split('__label__')[1][1:])\n",
    "    train_label_raw.append(line.split('__label__')[1][0])\n",
    "with open('../input/test.ft.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "for line in tqdm(lines):\n",
    "    test_raw.append(line.split('__label__')[1][1:])\n",
    "    test_label_raw.append(line.split('__label__')[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert data to a dataframe\n",
    "train = pd.DataFrame({'text': train_raw, 'target': train_label_raw})\n",
    "test = pd.DataFrame({'text': test_raw, 'target': test_label_raw})\n",
    "#turn targets into ints\n",
    "train['target'] = train['target'].astype(int)\n",
    "test['target'] = test['target'].astype(int)\n",
    "target_map = {1:0, 2:1}\n",
    "train['target'] = train['target'].replace(target_map)\n",
    "test['target'] = test['target'].replace(target_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class moved to classes.py\n",
    "'''class amazon_dataset(Dataset):\n",
    "    def __init__(self, encoded_data, labels):\n",
    "        self.input_ids = encoded_data[\"input_ids\"]\n",
    "        self.attention_mask = encoded_data[\"attention_mask\"]\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)  # Convert labels to tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set length for training data, this also impacts test length\n",
    "train_len = 1000\n",
    "\n",
    "train_text = train['text'].to_list()[:train_len]\n",
    "test_text = test['text'].to_list()[:int(train_len/8)]\n",
    "train_targets = train['target'].to_list()[:train_len]\n",
    "test_targets = test['target'].to_list()[:int(train_len/8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_targets, bins=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "time_dict = {}\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 32\n",
    "\n",
    "# Set params for grid search\n",
    "param_grid = {\n",
    "    'lr': [5e-5, 4e-5, 3e-5, 2e-5],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],\n",
    "    # Add more hyperparameters as needed\n",
    "}\n",
    "\n",
    "total_models_per_model = len(param_grid['lr']) * len(param_grid['dropout_rate'])\n",
    "\n",
    "id = 0\n",
    "\n",
    "for model_id in model_list:\n",
    "\n",
    "    print(model_id)\n",
    "\n",
    "    time_list = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if model_id == 'bert-base-uncased':\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "        model = BertForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "    elif model_id == 'distilbert-base-uncased':\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_id)\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "        \n",
    "    elif model_id == 'xlnet-base-uncased':\n",
    "        tokenizer = XLNetTokenizer.from_pretrained(model_id)\n",
    "        model = XLNetForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "\n",
    "    elif model_id == 'roberta-base':\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_id)\n",
    "        model = RobertaForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "    elif model_id == 'distilroberta-base':\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "\n",
    "    else:\n",
    "        raise Exception('Model not recognised')\n",
    "\n",
    "    #set the processor to the best avalible option\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(f'Using: {device}')\n",
    "    model.to(device)\n",
    "\n",
    "    progress_bar = tqdm(range(total_models_per_model))\n",
    "    #progress_bar = tqdm(len(train_dataloader))\n",
    "    \n",
    "    for lr in param_grid['lr']:\n",
    "        for dropout_rate in param_grid['dropout_rate']:\n",
    "\n",
    "            # Set your hyperparameters\n",
    "            learning_rate = lr\n",
    "            batch_size = batch_size\n",
    "            num_epochs = epochs\n",
    "            dropout_prob = dropout_rate\n",
    "\n",
    "            # Assuming your amazon_dataset class takes care of loading data and encoding\n",
    "            train_text_encoded = tokenizer.batch_encode_plus(train_text, add_special_tokens=True, truncation=True, padding=True, return_tensors='pt', max_length=128, return_attention_mask=True)\n",
    "            test_text_encoded = tokenizer.batch_encode_plus(test_text, add_special_tokens=True, truncation=True, padding=True, return_tensors='pt', max_length=128, return_attention_mask=True)\n",
    "\n",
    "            train_dataset = amazon_dataset(train_text_encoded, train_targets)\n",
    "            test_dataset = amazon_dataset(test_text_encoded, test_targets)\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "            # Define the optimizer and loss function\n",
    "            optimiser = AdamW(params=model.parameters(), lr=learning_rate)\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Train the current model\n",
    "            for epoch in range(epochs):\n",
    "                for batch in train_dataloader:\n",
    "                    inputs = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['labels'].to(device)\n",
    "\n",
    "                    optimiser.zero_grad()\n",
    "                    outputs = model(inputs, attention_mask=attention_mask, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                    loss.backward() \n",
    "                    optimiser.step()\n",
    "            end_time = time.time()\n",
    "\n",
    "            elapsed_time = end_time - start_time\n",
    "\n",
    "            time_list.append(elapsed_time)\n",
    "\n",
    "            # Evaluate the model on the test set and save the hyperparameters and performance\n",
    "            # (you need to implement the evaluation part based on your dataset and task)\n",
    "            train_preds, train_true, test_preds, test_true = [], [], [], []\n",
    "\n",
    "            # Make training set predictions\n",
    "            for batch in train_dataloader:\n",
    "                with torch.no_grad():\n",
    "                    inputs = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['labels'].to(device)\n",
    "\n",
    "                    model.eval()\n",
    "\n",
    "                    output = model(inputs, attention_mask=attention_mask)\n",
    "\n",
    "                    logits = output.logits\n",
    "                    predictions = torch.argmax(logits, dim=-1)\n",
    "                    predictions = predictions.cpu()\n",
    "\n",
    "                    train_preds.append(predictions.numpy())\n",
    "                    labels = labels.cpu()\n",
    "                    train_true.append(labels.numpy())\n",
    "            \n",
    "            # Make testing set predictions\n",
    "            for batch in test_dataloader:\n",
    "                with torch.no_grad():\n",
    "                    inputs = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['labels'].to(device)\n",
    "\n",
    "                    model.eval()\n",
    "\n",
    "                    output = model(inputs, attention_mask=attention_mask)\n",
    "\n",
    "                    logits = output.logits\n",
    "                    predictions = torch.argmax(logits, dim=-1)\n",
    "                    predictions = predictions.cpu()\n",
    "\n",
    "                    test_preds.append(predictions.numpy())\n",
    "                    labels = labels.cpu()\n",
    "                    test_true.append(labels.numpy())\n",
    "\n",
    "            # Flatten predictions\n",
    "            train_flat_preds = np.concatenate([batch_preds for batch_preds in train_preds])\n",
    "            train_flat_true = np.concatenate([batch_true for batch_true in train_true])\n",
    "            test_flat_preds = np.concatenate([batch_preds for batch_preds in test_preds])\n",
    "            test_flat_true = np.concatenate([batch_true for batch_true in test_true])\n",
    "\n",
    "            # Get the training and testing accuracies\n",
    "            train_accuracy = accuracy_score(train_flat_true, train_flat_preds)\n",
    "            test_accuracy = accuracy_score(test_flat_true, test_flat_preds)\n",
    "\n",
    "            print(f'lr={learning_rate}, batch_size={batch_size} dropout={dropout_prob}, train accuracy={train_accuracy}, test accuracy={test_accuracy}')\n",
    "\n",
    "            model_name = f'{str(model_id)}_{str(learning_rate)}_{str(batch_size)}_{str(dropout_prob)}'\n",
    "\n",
    "            # Save the results to a dictionary\n",
    "            results[id] = {'model':model_id, 'Learning Rate': learning_rate, 'Dropout Probability': dropout_prob, 'Train Accuracy': train_accuracy, 'test Accuracy': test_accuracy}\n",
    "\n",
    "            # Create directory for models\n",
    "            output_dir = f'../modelsV2/{model_name}'\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "\n",
    "            # Save models\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "            model_to_save.save_pretrained(output_dir)\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "            # Track progress of for loop\n",
    "            progress_bar.update(1)\n",
    "            id = id+1\n",
    "    time_dict[model_id] = sum(time_list)/len(time_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to dataframes\n",
    "results_df = pd.DataFrame(results)\n",
    "time_df = pd.DataFrame(time_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes as CSVs\n",
    "results_df.to_csv(f'results/base/resultsV{version}.csv')\n",
    "time_df.to_csv(f'results/base/timeV{version}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentimentAnalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
