{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, XLNetTokenizer, XLNetForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from classes import *\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert-base-uncased\n",
    "#distilbert-base-uncased\n",
    "#xlnet-base-uncased\n",
    "#roberta-base\n",
    "#distilroberta-base\n",
    "model_list = ['bert-base-uncased','distilbert-base-uncased',\n",
    "              'xlnet-base-uncased',\n",
    "              'roberta-base','distilroberta-base']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model and parameters and create lebel accordingly\n",
    "model = model_list[1]\n",
    "learning_rate = 2e-5\n",
    "model_name = f'{model}_lr{learning_rate}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert-base-uncased_lr2e-05\n"
     ]
    }
   ],
   "source": [
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "'''train_raw,test_raw,train_label_raw,test_label_raw=[],[],[],[]\n",
    "with open('../input/train.ft.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "for line in tqdm(lines):\n",
    "    train_raw.append(line.split('__label__')[1][1:])\n",
    "    train_label_raw.append(line.split('__label__')[1][0])\n",
    "with open('../input/test.ft.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "for line in tqdm(lines):\n",
    "    test_raw.append(line.split('__label__')[1][1:])\n",
    "    test_label_raw.append(line.split('__label__')[1][0])\n",
    "\n",
    "#convert data to a dataframe\n",
    "train = pd.DataFrame({'text': train_raw, 'target': train_label_raw})\n",
    "test = pd.DataFrame({'text': test_raw, 'target': test_label_raw})\n",
    "#turn targets into ints\n",
    "train['target'] = train['target'].astype(int)\n",
    "test['target'] = test['target'].astype(int)\n",
    "target_map = {1:0, 2:1}\n",
    "train['target'] = train['target'].replace(target_map)\n",
    "test['target'] = test['target'].replace(target_map)'''\n",
    "\n",
    "file = '../input/amazon_cells_labelled.txt'\n",
    "df = pd.read_csv(file, delimiter='\\t', header=None, names=['text', 'target'])\n",
    "\n",
    "train_length = int(len(df)*0.8)\n",
    "\n",
    "train = df[:train_length]\n",
    "test = df[train_length:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>Good , works fine.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>For the price this was a great deal.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>Great price, too!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>If there is a wind, it is completely useless.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>Yes it's shiny on front side - and I love it!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text  target\n",
       "800                             Good , works fine.       1\n",
       "801           For the price this was a great deal.       1\n",
       "802                              Great price, too!       1\n",
       "803  If there is a wind, it is completely useless.       0\n",
       "804  Yes it's shiny on front side - and I love it!       1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moved to classes.py\n",
    "'''class amazon_dataset(Dataset):\n",
    "    def __init__(self, encoded_data, labels):\n",
    "        self.input_ids = encoded_data[\"input_ids\"]\n",
    "        self.attention_mask = encoded_data[\"attention_mask\"]\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)  # Convert labels to tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set length for training data, this also impacts test length\n",
    "train_len = 800\n",
    "\n",
    "train_text = train['text'].to_list()[:train_len]\n",
    "test_text = test['text'].to_list()[:int(train_len/8)]\n",
    "train_targets = train['target'].to_list()[:train_len]\n",
    "test_targets = test['target'].to_list()[:int(train_len/8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_targets, bins=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Moved to functions.py\n",
    "'''def freeze(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Let's say we want to train only the classifier layer\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    return model'''\n",
    "\n",
    "# Load relevent model and freeze layers. Freezing will result in poor performace in this instance and so should be removed in future iterations. \n",
    "# Freezing was done to speed up training time to check the code runs smoothly\n",
    "if model == 'bert-base-uncased':\n",
    "    tokenizer = BertTokenizer.from_pretrained(model)\n",
    "    model = freeze(BertForSequenceClassification.from_pretrained(model, num_labels=2))\n",
    "elif model == 'distilbert-base-uncased':\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model, num_labels=2)\n",
    "    \n",
    "elif model == 'xlnet-base-uncased':\n",
    "    tokenizer = XLNetTokenizer.from_pretrained(model)\n",
    "    model = freeze(XLNetForSequenceClassification.from_pretrained(model, num_labels=2))\n",
    "\n",
    "elif model == 'roberta-base':\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model)\n",
    "    model = freeze(RobertaForSequenceClassification.from_pretrained(model, num_labels=2))\n",
    "elif model == 'distilroberta-base':\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "\n",
    "else:\n",
    "    raise Exception('Model not recognised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the text\n",
    "train_text_encoded = tokenizer.batch_encode_plus(train_text, add_special_tokens=True, truncation=True, padding=True, return_tensors='pt', max_length=128, return_attention_mask=True)\n",
    "test_text_encoded = tokenizer.batch_encode_plus(test_text, add_special_tokens=True, truncation=True, padding=True, return_tensors='pt', max_length=128, return_attention_mask=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load encoded text into datasets and then dataloaders ready for PyTorch\n",
    "train_dataset = amazon_dataset(train_text_encoded, train_targets)\n",
    "test_dataset = amazon_dataset(test_text_encoded, test_targets)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the processor to the best avalible option\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f'Using: {device}')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = AdamW(params=model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert-base-uncased_lr2e-05\n"
     ]
    }
   ],
   "source": [
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [26:39<00:00, 63.80s/it]"
     ]
    }
   ],
   "source": [
    "# base-bert-cased 391m\n",
    "\n",
    "# base-bert-uncased 387m\n",
    "#0.896\n",
    "#0.8976377952755905\n",
    "\n",
    "# distilbert-base-cased 200m\n",
    "#0.808\n",
    "#0.7894736842105263\n",
    "\n",
    "# Train the model\n",
    "epochs = 1\n",
    "\n",
    "num_training_steps = epochs * len(train_dataloader)\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_dataloader:\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward() \n",
    "        optimiser.step()\n",
    "        progress_bar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:41<00:00, 40.45s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create predictions\n",
    "preds, true = [], []\n",
    "\n",
    "for batch in tqdm(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        output = model(inputs, attention_mask=attention_mask)\n",
    "\n",
    "        logits = output.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        preds.append(predictions.numpy())\n",
    "\n",
    "        true.append(labels.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the preds\n",
    "flat_preds = np.concatenate([batch_preds for batch_preds in preds])\n",
    "flat_true = np.concatenate([batch_true for batch_true in true])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0\n",
      " 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0\n",
      " 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1]\n",
      "[1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 0 1 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(flat_preds)\n",
    "print(flat_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Poor results due to freezing\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "print(accuracy_score(flat_true, flat_preds))\n",
    "print(f1_score(flat_true, flat_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/distilbert-base-uncased_lr2e-05\\\\tokenizer_config.json',\n",
       " '../models/distilbert-base-uncased_lr2e-05\\\\special_tokens_map.json',\n",
       " '../models/distilbert-base-uncased_lr2e-05\\\\vocab.txt',\n",
       " '../models/distilbert-base-uncased_lr2e-05\\\\added_tokens.json')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create directory and save model\n",
    "\n",
    "import os\n",
    "\n",
    "output_dir = f'../models/{model_name}'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentimentAnalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
