{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, DistilBertForSequenceClassification\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functions import getdata\n",
    "from classes import amazon_dataset, amazon_dataset_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model\n",
    "model = 'distilbert-base-uncased_2e-05_32_0.3'\n",
    "\n",
    "model_path = f'../modelsV2/dis/{model}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get web data if you'd rather\n",
    "'''options = webdriver.ChromeOptions()\n",
    "driver_path = ChromeDriverManager().install()\n",
    "\n",
    "reviews = getdata('https://www.amazon.com/product-reviews/B0828BJGD2/',options,driver_path)\n",
    "\n",
    "df = pd.Series(reviews)'''\n",
    "\n",
    "#get csv data, edit these as needed\n",
    "bose = pd.read_csv(\"\")\n",
    "sony = pd.read_csv(\"\")\n",
    "yuandidu = pd.read_csv(\"\")\n",
    "\n",
    "#if you want to use all the data it can be concatinated\n",
    "'''df = pd.concat([bose,sony,yuandidu])\n",
    "print(len(bose), len(sony), len(yuandidu),len(df),len(bose)+len(sony)+len(yuandidu))\n",
    "\n",
    "print(df.info())\n",
    "reviews = df[['text','rating']]\n",
    "\n",
    "bose_reviews = reviews[reviews['text'].str.contains('bose', case=False)]\n",
    "print(len(bose_reviews))'''\n",
    "\n",
    "df = sony[['text','rating']]\n",
    "\n",
    "reviews = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rating'].hist(bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stopwords and create wordcloud\n",
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color='black', max_words=100, width=800, height=400).generate(list(reviews)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_text = \" \".join(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is avalible, if not use SPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f'Using: {device}')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the paragraphs that \n",
    "def extract_relevant_paragraphs(text, keywords):\n",
    "    text = ' '.join(text)\n",
    "    instances = text.split('.')\n",
    "    relevant_instances = []\n",
    "\n",
    "    # Loop through each sentence (limited at 50 just for testing speed)\n",
    "    for instance in instances[:50]:\n",
    "        # Check if any of the keywords are in the sentence\n",
    "        if keywords[0] == 'total':\n",
    "            relevant_instances.append(instance.strip())\n",
    "        else:\n",
    "            if any(keyword.lower() in instance.lower() for keyword in keywords):\n",
    "                relevant_instances.append(instance.strip())\n",
    "\n",
    "    return relevant_instances\n",
    "\n",
    "features = ['total', 'bluetooth', 'ANC']\n",
    "survey_dict = {}\n",
    "\n",
    "# Get relevent paragraphs for each target feature and create model sentiment predictions\n",
    "for feature in features:\n",
    "    relevant_instances = extract_relevant_paragraphs(reviews, [feature])\n",
    "    print(len(relevant_instances))\n",
    "\n",
    "    # Only continue if there are enough datapoints\n",
    "    if len(relevant_instances) > 4:\n",
    "\n",
    "        reviews_encoded = tokenizer.batch_encode_plus(relevant_instances, add_special_tokens=True, truncation=True, padding=True, return_tensors='pt', max_length=128, return_attention_mask=True)\n",
    "        dataset = amazon_dataset_run(reviews_encoded, labels=None)\n",
    "        dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "        preds = []\n",
    "\n",
    "        for batch in tqdm(dataloader):\n",
    "            with torch.no_grad():\n",
    "                inputs = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "                model.eval()\n",
    "\n",
    "                output = model(inputs, attention_mask=attention_mask)\n",
    "\n",
    "                logits = output.logits\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "                preds.append(predictions.numpy())\n",
    "        for i in range(len(relevant_instances)):\n",
    "            print(preds[i], relevant_instances[i])\n",
    "        \n",
    "        print(float(sum(preds)/len(preds)))\n",
    "\n",
    "        # Add results to dictionary\n",
    "        survey_dict[feature] = float(sum(preds)/len(preds))\n",
    "    else:\n",
    "        # Label as not enough datapoints if there are less points than specified\n",
    "        survey_dict[feature] = f'Not enough data: {len(relevant_instances)} datapoints found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(survey_dict)\n",
    "results_df = pd.DataFrame([survey_dict]).T\n",
    "results_df.columns = ['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_df.to_csv('C:/Users/UKGC/Sensian Research Ltd/Overall Data Sharing - Documents/Python Scripts/Sentiment analysis/bose_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentimentAnalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
